{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os.path\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import nn, optim\n",
    "from skorch.callbacks import EarlyStopping, EpochScoring\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the current directory if __file__ is not available\n",
    "current_dir = os.getcwd()  # Gets the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))  # Moves one level up\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the parent directory\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Move one level up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose the phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype = \"YPD_doublingtime\"\n",
    "#phenotype = \"YPDCUSO410MM_40h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = os.path.join(data_dir, f\"X_matrix_restricted_{phenotype}.pkl\")\n",
    "Y_file = os.path.join(data_dir, f\"data/y_{phenotype}.csv\")\n",
    "\n",
    "\n",
    "x2_df = pd.read_pickle(os.path.join(data_dir, f\"data/X_matrix_restricted_{phenotype}.pkl\"))\n",
    "\n",
    "\n",
    "print(\"moving to y\")\n",
    "y2_df = pd.read_csv(Y_file)\n",
    "\n",
    "x_data_f = x2_df.drop(x2_df.columns[0], axis=1)\n",
    "y_data_f = y2_df.drop(y2_df.columns[0], axis=1)\n",
    "\n",
    "x_data_f, y_data_f = shuffle_dataset(x_data_f, y_data_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input features\n",
    "n_input_features = x_data_f.shape[1]\n",
    "\n",
    "# Enhanced Neural Network with More Layers and Neurons\n",
    "class EnhancedRegressionNet(nn.Module):\n",
    "    def __init__(self, n_input_features, dropout_rate, n_neurons_1=1024, n_neurons_2=512, n_neurons_3=256, n_neurons_4=256):\n",
    "        super(EnhancedRegressionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input_features, n_neurons_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(n_neurons_1, n_neurons_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(n_neurons_2, n_neurons_3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(n_neurons_3, 1)  # Output layer remains the same\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Ensure both x_data_f and y_data_f are converted to float32\n",
    "x_data_f = x_data_f.astype(np.float32)  # Cast features to float32\n",
    "y_data_f = y_data_f.astype(np.float32)  # Cast target to float32\n",
    "\n",
    "# Define scoring callbacks for training and validation loss\n",
    "train_loss = EpochScoring(scoring='neg_mean_squared_error', on_train=True, name='train_loss', lower_is_better=False)\n",
    "valid_loss = EpochScoring(scoring='neg_mean_squared_error', name='valid_loss', lower_is_better=False)\n",
    "\n",
    "# Neural Network Regressor\n",
    "net = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features,  # n_input_features\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    optimizer__weight_decay=1e-5,               # L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'module__dropout_rate': [0.01, 0.1, 0.2],\n",
    "    'lr': [ 0.0001, 0.00001],\n",
    "    'max_epochs': [100, 150]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(net, param_grid=param_grid, cv=KFold(n_splits=5), scoring='neg_mean_squared_error', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_f = y_data_f.to_numpy()\n",
    "print(y_data_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN, the best parameters are saved\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(x_data_f.values, y_data_f)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'module__dropout_rate': 0.3,  # Single value for dropout rate\n",
    "    'lr': 0.0001,                # Single value for learning rate\n",
    "    'max_epochs': 150            # Single value for max epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add L2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the Model with More Complexity and L2 Regularization\n",
    "best_net2 = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features,\n",
    "    module__n_neurons_1=2048,\n",
    "    module__n_neurons_2=1024,                  \n",
    "    module__n_neurons_3=512,\n",
    "    module__dropout_rate=best_params['module__dropout_rate'],\n",
    "    criterion=nn.MSELoss,\n",
    "    max_epochs=best_params['max_epochs'],\n",
    "    optimizer=optim.Adam,\n",
    "    lr=best_params['lr'],\n",
    "    optimizer__weight_decay=5e-4,              # L2 regularization (Weight Decay)\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5), train_loss, valid_loss],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the Final Model\n",
    "best_net2.fit(x_data_f.values, y_data_f)\n",
    "\n",
    "# Predictions\n",
    "Y_pred = best_net2.predict(x_data_f.values)\n",
    "\n",
    "# Reshape Predictions (if needed)\n",
    "Y_pred = Y_pred.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_nn = best_net2\n",
    "\n",
    "# Use the NN to generate predictions\n",
    "#y_pred = trained_nn.predict(x_data_f.values)\n",
    "y_pred = Y_pred\n",
    "\n",
    "# Train a Random Forest on the original input features\n",
    "rf = RandomForestRegressor(max_depth=40, \n",
    "                           max_features='sqrt', \n",
    "                           min_samples_leaf = 4, \n",
    "                           n_estimators=500, \n",
    "                           random_state=42)\n",
    "rf.fit(x_data_f, y_pred)\n",
    "\n",
    "# Display feature importances from Random Forest\n",
    "importances = rf.feature_importances_\n",
    "for feature_name, importance in zip(x_data_f.columns, importances):\n",
    "    print(f\"Feature: {feature_name}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature importances in a CSV\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': x_data_f.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by 'Importance' in descending order\n",
    "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Save the sorted DataFrame to a CSV file\n",
    "importances_df.to_csv('../results/mutations_NNrestricted_importance.csv', index=False)\n",
    "\n",
    "print(\"Feature importances saved in descending order of importance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances and names\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_names = x_data_f.columns\n",
    "\n",
    "# Combine feature names and their importances\n",
    "features = list(zip(feature_names, feature_importances))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(features, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 20 most important features\n",
    "top_20_features = sorted_features[:20]\n",
    "\n",
    "# Split into names and values for plotting\n",
    "top_20_names, top_20_values = zip(*top_20_features)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_20_names, top_20_values, color='skyblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
