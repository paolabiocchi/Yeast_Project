{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the current directory if __file__ is not available\n",
    "current_dir = os.getcwd()  # Gets the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))  # Moves one level up\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Move one level up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose the phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype = \"YPD_doublingtime\"\n",
    "#phenotype = \"YPDCUSO410MM_40h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = os.path.join(data_dir, f\"data/X_matrix_{phenotype}.csv\")\n",
    "Y_file = os.path.join(data_dir, f\"data/y_{phenotype}.csv\")\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "x_df = pd.read_csv(X_file)\n",
    "y_df = pd.read_csv(Y_file)\n",
    "x_data_f, y_data_f = preprocessed_data(x_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation des données...\n",
      "\n",
      "Input format:  torch.Size([792, 341957]) torch.float32\n",
      "Output format:  torch.Size([792]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing the data...\")\n",
    "\n",
    "x = x_data_f.drop(columns=[\"Yeast_ID\"]).fillna(0) \n",
    "y = y_data_f[\"YPD_doublingtime\"].fillna(y_data_f[\"YPD_doublingtime\"].mean())\n",
    "\n",
    "x = x.values.astype('float32')  \n",
    "y = y.values.flatten().astype('int64')  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input = torch.tensor(x)  \n",
    "print('\\nInput format: ', input.shape, input.dtype)\n",
    "\n",
    "output = torch.tensor(y)     \n",
    "print('Output format: ', output.shape, output.dtype)\n",
    "\n",
    "# Create a TensorDataset for easy data handling in PyTorch\n",
    "data = TensorDataset(input, output)  # Combine input and output tensors into a single dataset object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to Train, Validate and Test sets using random_split\n",
    "train_batch_size = 8\n",
    "number_rows = len(input)\n",
    "test_split = int(number_rows * 0.3)\n",
    "validate_split = int(number_rows * 0.2)\n",
    "train_split = number_rows - test_split - validate_split\n",
    "train_set, validate_set, test_set = random_split(data, [train_split, validate_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader to read the data within batch sizes and put into memory\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_set, batch_size=1)\n",
    "test_loader = DataLoader(test_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341957 1\n",
      "The model will be running on cpu device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "input_size = list(input.shape)[1]\n",
    "learning_rate = 0.025\n",
    "output_size = 1\n",
    "\n",
    "print(input_size, output_size)\n",
    "\n",
    "# Define neural network\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network with configurable layer sizes and dropout.\n",
    "\n",
    "    Parameters:\n",
    "        layer_sizes (list): A list of integers specifying the sizes of each layer, \n",
    "                            including input, hidden, and output layers.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list containing Linear layers and Dropout layers.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs the forward pass through the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Add dropout to intermediate layers\n",
    "                self.layers.append(nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, input_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all layers.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.relu(x)  # Activation after Linear layers\n",
    "        return x\n",
    "\n",
    "layer_sizes = [input_size, 7500, 7000, 6000, 5000, output_size]\n",
    "model = Network(layer_sizes)      \n",
    "\n",
    "# Define your execution device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\\n\")\n",
    "model.to(device)\n",
    "\n",
    "def saveModel():\n",
    "    \"\"\"\n",
    "    Save the current model's state dictionary to a file.\n",
    "    \n",
    "    The file is named based on the 'phenotype' variable, \n",
    "    ensuring clarity for which phenotype the model was trained.\n",
    "\n",
    "    File format: \"./NN_regmat_PA_<phenotype>.pth\"\n",
    "    \"\"\"\n",
    "    path = f\"./NN_regmat_PA_{phenotype}.pth\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function with MSE loss and an optimizer with Adam optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0005, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    \"\"\"\n",
    "    Train the neural network model using training and validation datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        num_epochs (int): Number of epochs to train the model.\n",
    "    \n",
    "    Returns:\n",
    "        predicted_outputs (torch.Tensor): Predictions from the final model on the validation set.\n",
    "        outputs (torch.Tensor): Actual target values from the validation set.\n",
    "        running_accuracy (float): Cumulative Pearson correlation coefficient across validation batches.\n",
    "    \"\"\"\n",
    "    best_accuracy = 0.0\n",
    "    val_epoch = 5\n",
    "    print(\"Begin training...\")\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        running_train_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for data in train_loader:\n",
    "            # For data in enumerate(train_loader, 0):\n",
    "            inputs, outputs = data              # Get the input and real species as outputs; data is a list of [inputs, outputs]\n",
    "            inputs = inputs.float().to(device)\n",
    "            outputs = outputs.float().to(device)\n",
    "            optimizer.zero_grad()               # Zero the parameter gradients\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            predicted_outputs = model(inputs)   # Predict output from the model\n",
    "\n",
    "            outputs = outputs.view(-1,1)\n",
    "            train_loss = loss_fn(predicted_outputs, outputs)    # Calculate loss for the predicted output\n",
    "\n",
    "            train_loss.backward()               # Backpropagate the loss\n",
    "\n",
    "            optimizer.step()                    # Adjust parameters based on the calculated gradients\n",
    "            running_train_loss += train_loss.item()             # Track the loss value\n",
    "\n",
    "        train_loss_value = running_train_loss / len(train_loader)\n",
    "        if (epoch % val_epoch == 0) and (epoch > 0):\n",
    "            # Validation Loop\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for data in validate_loader:\n",
    "                    inputs, outputs = data\n",
    "                    inputs = inputs.float()\n",
    "                    outputs = outputs.float()\n",
    "                    predicted_outputs = model(inputs)\n",
    "                    outputs.view(-1,1)\n",
    "                    val_loss = loss_fn(predicted_outputs, outputs)\n",
    "\n",
    "                    # The label with the highest value will be our prediction\n",
    "                    running_vall_loss += val_loss.item()\n",
    "                    total += outputs.size(0)\n",
    "                    running_accuracy += (pearsonr(outputs[0].cpu().detach().numpy(), predicted_outputs[0].cpu().detach().numpy())[0])\n",
    "\n",
    "            # Calculate validation loss value\n",
    "            val_loss_value = running_vall_loss / len(validate_loader)\n",
    "\n",
    "            # Calculate accuracy as the number of average Pearson's coefficient in the validation batch divided by the total number of predictions done.\n",
    "            accuracy = (100 * running_accuracy / total)\n",
    "\n",
    "            # Save the model if the accuracy is the best\n",
    "            if accuracy > best_accuracy:\n",
    "                saveModel()\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "            # Print the statistics of the epoch\n",
    "            print('Completed training batch', epoch, 'Training Loss is: %4f' % train_loss_value,\n",
    "                  'Validation Loss is: %.4f' % val_loss_value, 'Accuracy is %d %%' % accuracy)\n",
    "\n",
    "    return predicted_outputs, outputs, running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"\n",
    "    Test the trained model on the test dataset and calculate its performance.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the pre-trained model.\n",
    "        2. Run inference on the test data.\n",
    "        3. Compute accuracy based on Pearson correlation.\n",
    "        4. Return the ground truth outputs and predicted outputs.\n",
    "\n",
    "    Returns:\n",
    "        output_matrix (list): List of true output tensors.\n",
    "        predicted_output_matrix (list): List of predicted output tensors.\n",
    "    \"\"\"\n",
    "    # Load the model that we saved at the end of the training loop\n",
    "    model = Network(input_size, output_size)\n",
    "    path = \"./NN_regmat_PA.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    predicted_output_matrix = []\n",
    "    output_matrix = []\n",
    "    running_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, outputs = data\n",
    "            inputs = inputs.float()\n",
    "            outputs = outputs.float()\n",
    "            output_matrix.append(outputs)\n",
    "            predicted_outputs = model(inputs)\n",
    "            predicted_output_matrix.append(predicted_outputs)\n",
    "            _, predicted = torch.max(predicted_outputs, 1)\n",
    "            total += outputs.size(0)\n",
    "            running_accuracy += pearsonr(outputs[0].cpu().detach().numpy(),predicted_outputs[0].cpu().detach().numpy())[0]\n",
    "\n",
    "        print('Accuracy of the model based on the test set of', test_split,\n",
    "              'inputs is: %d %%' % (100 * running_accuracy / total))\n",
    "\n",
    "    return output_matrix, predicted_output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Training Loop\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# for data in enumerate(train_loader, 0):\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# get the input and real species as outputs; data is a list of [inputs, outputs]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_output = train(num_epochs)\n",
    "print('Finished Training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test function to get model outputs\n",
    "test_output = test()\n",
    "\n",
    "# Convert outputs and predictions from tensors to lists\n",
    "for k in range(len(test_output[0])):\n",
    "    test_output[0][k] = test_output[0][k].tolist()[0]  # Convert true outputs\n",
    "    test_output[1][k] = test_output[1][k].tolist()[0]  # Convert predicted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.reshape(test_output[0], (test_split, output_size)).T  \n",
    "test_matrix_predicted = np.reshape(test_output[1], (test_split, output_size)).T  \n",
    "\n",
    "pd.DataFrame(test_matrix).to_csv('Test_matrix_true_using_PA.csv', index=False) \n",
    "pd.DataFrame(test_matrix_predicted).to_csv('Test_matrix_predicted_using_PA.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_full_data(model, x_data, device):\n",
    "    \"\"\"\n",
    "    Generate predictions on the full dataset using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained neural network model.\n",
    "        x_data (pd.DataFrame): Input feature data for prediction.\n",
    "        device (torch.device): Device to run the model (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions.\n",
    "    \"\"\"\n",
    "    model.eval()                        \n",
    "    x_data = torch.tensor(x_data.values, dtype=torch.float32).to(device)  \n",
    "    with torch.no_grad():               \n",
    "        predictions = model(x_data)     # Generate predictions\n",
    "    return predictions.cpu().numpy()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, targets):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance using Mean Squared Error (MSE) and R-squared (R2) metrics.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (array-like): Predicted values from the model.\n",
    "        targets (array-like): Ground truth target values.\n",
    "\n",
    "    Returns:\n",
    "        mse (float): Mean Squared Error between predictions and targets.\n",
    "        r2 (float): R-squared score representing the goodness of fit.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(targets, predictions)  # Calculate Mean Squared Error\n",
    "    r2 = r2_score(targets, predictions)             # Calculate R-squared score\n",
    "    return mse, r2\n",
    "\n",
    "# Compute metrics\n",
    "mse, r2 = evaluate_model(predictions, y_train.values)\n",
    "print(f\"MSE: {mse}, R²: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
