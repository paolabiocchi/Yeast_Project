{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the current directory if __file__ is not available\n",
    "current_dir = os.getcwd()  # Gets the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))  # Moves one level up\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m Y_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Y_matrix.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading the data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m x_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Y_file)\n\u001b[0;32m      7\u001b[0m x_data_f, y_data_f \u001b[38;5;241m=\u001b[39m preprocessed_data(x_df, y_df)\n",
      "File \u001b[1;32mc:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# Define the path to the parent directory\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Move one level up\n",
    "\n",
    "X_file = os.path.join(data_dir, \"data/X_matrix.csv\")\n",
    "Y_file = os.path.join(data_dir, \"data/Y_matrix.csv\")\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "x_df = pd.read_csv(X_file)\n",
    "y_df = pd.read_csv(Y_file)\n",
    "x_data_f, y_data_f = preprocessed_data(x_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation des données...\n",
      "\n",
      "Input format:  torch.Size([792, 341957]) torch.float32\n",
      "Output format:  torch.Size([792]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Convert Input and Output data to Tensors and create a TensorDataset\n",
    "# Wrap the input and output tensors into a single dataset object\n",
    "\n",
    "# Préparation des données\n",
    "print(\"Préparation des données...\")\n",
    "x = x_data_f.drop(columns=[\"Yeast_ID\"]).fillna(0)  # Remplacer les valeurs manquantes par 0 dans X\n",
    "y = y_data_f[\"YPD_doublingtime\"].fillna(y_data_f[\"YPD_doublingtime\"].mean())  # Remplacer les valeurs manquantes par la moyenne dans Y\n",
    "\n",
    "# Ensure x_data_f is entirely numeric and of dtype float32\n",
    "x = x.values.astype('float32')  # Force conversion to float32\n",
    "\n",
    "# Ensure y_data_f is entirely numeric and of dtype int64\n",
    "y = y.values.flatten().astype('int64')  # Flatten and force conversion to int64\n",
    "\n",
    "input = torch.tensor(x)      # type torch.float32\n",
    "print('\\nInput format: ', input.shape, input.dtype)\n",
    "output = torch.tensor(y)       # type torch.int64\n",
    "print('Output format: ', output.shape, output.dtype)\n",
    "data = TensorDataset(input, output)    # Create a torch.utils.data.TensorDataset object for further data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to Train, Validate and Test sets using random_split\n",
    "train_batch_size = 8\n",
    "number_rows = len(input)\n",
    "test_split = int(number_rows * 0.3)\n",
    "validate_split = int(number_rows * 0.2)\n",
    "train_split = number_rows - test_split - validate_split\n",
    "train_set, validate_set, test_set = random_split(data, [train_split, validate_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader to read the data within batch sizes and put into memory\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_set, batch_size=1)\n",
    "test_loader = DataLoader(test_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341957 1\n",
      "The model will be running on cpu device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "input_size = list(input.shape)[1]\n",
    "learning_rate = 0.025\n",
    "output_size = 1\n",
    "\n",
    "print(input_size, output_size)\n",
    "\n",
    "# Define neural network\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Add dropout to intermediate layers\n",
    "                self.layers.append(nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.relu(x)  # Activation after Linear layers\n",
    "        return x\n",
    "\n",
    "\n",
    "'''\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "        # Define layers with Linear and Dropout\n",
    "        self.layer1 = nn.Linear(input_size, 7500)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout after layer1\n",
    "\n",
    "        self.layer2 = nn.Linear(7500, 7000)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout after layer2\n",
    "\n",
    "        self.layer3 = nn.Linear(7000, 6000)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # Dropout after layer3\n",
    "\n",
    "        self.layer4 = nn.Linear(6000, 5000)\n",
    "        self.dropout4 = nn.Dropout(0.5)  # Dropout after layer4\n",
    "\n",
    "        self.final_layer = nn.Linear(5000, output_size)  # Final output layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass through layer1\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Pass through layer2\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Pass through layer3\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Pass through layer4\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Final layer (no activation here for regression)\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "layer_sizes = [input_size, 7500, 7000, 6000, 5000, output_size]\n",
    "model = Network(layer_sizes)      \n",
    "\n",
    "# Define your execution device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\\n\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to save the model\n",
    "def saveModel():\n",
    "    path = \"./NN_regmat_PA.pth\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function with MSE loss and an optimizer with Adam optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0005, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training Function\n",
    "def train(num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    val_epoch = 5\n",
    "    print(\"Begin training...\")\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        running_train_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for data in train_loader:\n",
    "            # for data in enumerate(train_loader, 0):\n",
    "            inputs, outputs = data  # get the input and real species as outputs; data is a list of [inputs, outputs]\n",
    "            inputs = inputs.float().to(device)\n",
    "            outputs = outputs.float().to(device)\n",
    "            optimizer.zero_grad()  # zero the parameter gradients\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            predicted_outputs = model(inputs)  # predict output from the model\n",
    "\n",
    "            outputs = outputs.view(-1,1)\n",
    "            train_loss = loss_fn(predicted_outputs, outputs)  # calculate loss for the predicted output\n",
    "\n",
    "            train_loss.backward()  # backpropagate the loss\n",
    "\n",
    "            optimizer.step()  # adjust parameters based on the calculated gradients\n",
    "            running_train_loss += train_loss.item()  # track the loss value\n",
    "        # Calculate training loss value\n",
    "\n",
    "        train_loss_value = running_train_loss / len(train_loader)\n",
    "        if (epoch % val_epoch == 0) and (epoch > 0):\n",
    "            # Validation Loop\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for data in validate_loader:\n",
    "                    inputs, outputs = data\n",
    "                    inputs = inputs.float()\n",
    "                    outputs = outputs.float()\n",
    "                    predicted_outputs = model(inputs)\n",
    "                    outputs.view(-1,1)\n",
    "                    val_loss = loss_fn(predicted_outputs, outputs)\n",
    "\n",
    "                # The label with the highest value will be our prediction\n",
    "                    running_vall_loss += val_loss.item()\n",
    "                    total += outputs.size(0)\n",
    "                    running_accuracy += (pearsonr(outputs[0].cpu().detach().numpy(), predicted_outputs[0].cpu().detach().numpy())[0])\n",
    "\n",
    "                # Calculate validation loss value\n",
    "            val_loss_value = running_vall_loss / len(validate_loader)\n",
    "\n",
    "            # Calculate accuracy as the number of average Pearson's coefficient in the validation batch divided by the total number of predictions done.\n",
    "            accuracy = (100 * running_accuracy / total)\n",
    "\n",
    "            # Save the model if the accuracy is the best\n",
    "            if accuracy > best_accuracy:\n",
    "                saveModel()\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "            # Print the statistics of the epoch\n",
    "            print('Completed training batch', epoch, 'Training Loss is: %4f' % train_loss_value,\n",
    "                  'Validation Loss is: %.4f' % val_loss_value, 'Accuracy is %d %%' % accuracy)\n",
    "\n",
    "    return predicted_outputs, outputs, running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test():\n",
    "    # Load the model that we saved at the end of the training loop\n",
    "    model = Network(input_size, output_size)\n",
    "    path = \"./NN_regmat_PA.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    predicted_output_matrix = []\n",
    "    output_matrix = []\n",
    "    running_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, outputs = data\n",
    "            inputs = inputs.float()\n",
    "            outputs = outputs.float()\n",
    "            output_matrix.append(outputs)\n",
    "            predicted_outputs = model(inputs)\n",
    "            predicted_output_matrix.append(predicted_outputs)\n",
    "            _, predicted = torch.max(predicted_outputs, 1)\n",
    "            total += outputs.size(0)\n",
    "            running_accuracy += pearsonr(outputs[0].cpu().detach().numpy(),predicted_outputs[0].cpu().detach().numpy())[0]\n",
    "\n",
    "        print('Accuracy of the model based on the test set of', test_split,\n",
    "              'inputs is: %d %%' % (100 * running_accuracy / total))\n",
    "\n",
    "    return output_matrix, predicted_output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Training Loop\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# for data in enumerate(train_loader, 0):\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# get the input and real species as outputs; data is a list of [inputs, outputs]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_output = train(num_epochs)\n",
    "print('Finished Training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test()\n",
    "\n",
    "\n",
    "for k in range(0, len(test_output[0])):\n",
    "    test_output[0][k] = test_output[0][k].tolist()[0]\n",
    "    test_output[1][k] = test_output[1][k].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.reshape(test_output[0], (test_split, output_size)).T\n",
    "test_matrix_predicted = np.reshape(test_output[1], (test_split, output_size)).T\n",
    "\n",
    "pd.DataFrame(test_matrix).to_csv('Test_matrix_true_using_PA.csv')\n",
    "pd.DataFrame(test_matrix_predicted).to_csv('Test_matrix_predicted_using_PA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_full_data(model, x_data, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    x_data = torch.tensor(x_data.values, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_data)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(predictions, targets):\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    return mse, r2\n",
    "\n",
    "# Compute metrics\n",
    "mse, r2 = evaluate_model(predictions, y_train.values)\n",
    "print(f\"MSE: {mse}, R²: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
