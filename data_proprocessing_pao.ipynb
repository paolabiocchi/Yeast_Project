{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv read complete\n",
      "Are the first columns the same? True\n"
     ]
    }
   ],
   "source": [
    "# y prends la forme : yeast ID, doubling_time\n",
    "# Charger les datasets\n",
    "x = pd.read_csv(\"data/X_matrix.csv\")\n",
    "y = pd.read_csv(\"data/Y_matrix.csv\")\n",
    "print(\"csv read complete\")\n",
    "\n",
    "#check if the first columns is the same\n",
    "is_same = x.iloc[:, 0].equals(y.iloc[:, 0])\n",
    "print(f\"Are the first columns the same? {is_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(data, title, bins=50, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Plot histograms of all numerical columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    data.hist(bins=bins, figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(data, title, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the correlation matrix.\n",
    "    \"\"\"\n",
    "    corr_matrix = data.corr()\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, annot=False, fmt=\".2f\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_pca_variance(pca, title):\n",
    "    \"\"\"\n",
    "    Plot the explained variance ratio of PCA components.\n",
    "    \"\"\"\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(explained_variance, marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_last_columns(data, num_last_columns=7000):\n",
    "    \"\"\"\n",
    "    Scales the last N columns (assumed to be CNVs) to a range of 0 to 1.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): x_train data\n",
    "        num_last_columns (int): Number of copy number variation columns from the end to scale .\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with the last columns scaled.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    cnv_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Scale these columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_cnv = scaler.fit_transform(cnv_columns)\n",
    "    \n",
    "    # Replace the last N columns with their scaled values\n",
    "    data.iloc[:, -num_last_columns:] = scaled_cnv\n",
    "\n",
    "    # Plot after scaling\n",
    "    #plot_histograms(data.iloc[:, -num_last_columns:], title=\"After Scaling CNVs\")\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_low_variance_features_last_columns(data, num_last_columns=7000, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Removes features with variance below a specified threshold in the last N columns.\n",
    "    This has been done already for mutations during our extraction of data, so it is only useful to do it for the copy number variation columns.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply variance filtering.\n",
    "        threshold (float): Minimum variance a feature must have to be retained.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with low-variance features removed in the last N columns.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply VarianceThreshold to these columns\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    reduced_data = selector.fit_transform(target_columns)\n",
    "    \n",
    "    # Get the selected column indices\n",
    "    selected_columns = target_columns.columns[selector.get_support()]\n",
    "    \n",
    "    # Replace the last N columns with the reduced set\n",
    "    data = data.drop(columns=target_columns.columns)  # Drop the original last N columns\n",
    "    data = pd.concat([data, pd.DataFrame(reduced_data, columns=selected_columns)], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Plot variance after filtering\n",
    "    reduced_variances = pd.DataFrame(reduced_data).var(axis=0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(reduced_variances, bins=50, color=\"orange\")\n",
    "    plt.title(\"Variance of Last Columns (After Filtering)\")\n",
    "    plt.xlabel(\"Variance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_pca_last_columns(data, num_last_columns=7000, n_components=0.95, normalize=True):\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce dimensionality of the last N columns of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply PCA.\n",
    "        n_components (float or int): Number of components to keep or the amount of variance to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with PCA applied to the last N columns.\n",
    "    \"\"\"\n",
    "    # Separate the last N columns and the rest of the dataset\n",
    "    other_columns = data.iloc[:, :-num_last_columns]\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply PCA to the last N columns\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(target_columns)\n",
    "\n",
    "    # Plot explained variance\n",
    "    #visualize_pca_variance(pca, title=\"Explained Variance by PCA Components\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Normalize the PCA-transformed features if specified\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        reduced_data = scaler.fit_transform(reduced_data)\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame for the reduced PCA data\n",
    "    reduced_columns = [f'PCA_{i+1}' for i in range(reduced_data.shape[1])]\n",
    "    reduced_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=data.index)\n",
    "    \n",
    "    # Concatenate the other columns with the reduced PCA columns\n",
    "    result = pd.concat([other_columns, reduced_df], axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def metrics(x_train, x_train_preprocessed):\n",
    "    \"\"\"\n",
    "    Compute and display various metrics for the original and preprocessed data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (pd.DataFrame): Original data.\n",
    "        x_train_preprocessed (pd.DataFrame): Preprocessed data.\n",
    "    \"\"\"\n",
    "    # Compute and print dimensions\n",
    "    print(\"Dimensions of the DataFrame (original):\", x_train.shape)\n",
    "    print(\"Dimensions of the DataFrame (after preprocessing):\", x_train_preprocessed.shape)\n",
    "\n",
    "    # Exclude the first row and column for calculations\n",
    "    x_cut = x_train.iloc[1:, 1:]\n",
    "    x_cut_p = x_train_preprocessed.iloc[1:, 1:]\n",
    "\n",
    "    # Compute and print mean\n",
    "    mean_value_x_train = x_cut.values.mean()\n",
    "    mean_value_x_train_preprocessed = x_cut_p.values.mean()\n",
    "    print(\"\\nMean of all values (original):\", mean_value_x_train)\n",
    "    print(\"Mean of all values (after preprocessing):\", mean_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print max and min\n",
    "    max_value_x_train = np.max(x_cut.values)\n",
    "    min_value_x_train = np.min(x_cut.values)\n",
    "    max_value_x_train_preprocessed = np.max(x_cut_p.values)\n",
    "    min_value_x_train_preprocessed = np.min(x_cut_p.values)\n",
    "    print(\"\\nMax value (original):\", max_value_x_train, \"Min value (original):\", min_value_x_train)\n",
    "    print(\"Max value (after preprocessing):\", max_value_x_train_preprocessed, \"Min value (after preprocessing):\", min_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print standard deviation\n",
    "    std_x_train = x_cut.values.std()\n",
    "    std_x_train_preprocessed = x_cut_p.values.std()\n",
    "    print(\"\\nStandard deviation (original):\", std_x_train)\n",
    "    print(\"Standard deviation (after preprocessing):\", std_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print variance\n",
    "    var_x_train = x_cut.values.var()\n",
    "    var_x_train_preprocessed = x_cut_p.values.var()\n",
    "    print(\"\\nVariance (original):\", var_x_train)\n",
    "    print(\"Variance (after preprocessing):\", var_x_train_preprocessed)\n",
    "\n",
    "\n",
    "def shuffle_dataset(data, labels):\n",
    "    \"\"\"\n",
    "    Shuffle the dataset and labels together to randomize the order of examples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Features dataset.\n",
    "        labels (pd.DataFrame): Labels dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Shuffled features and labels.\n",
    "    \"\"\"\n",
    "    combined = pd.concat([data, labels], axis=1)\n",
    "    shuffled = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Separate features and labels again\n",
    "    shuffled_data = shuffled.iloc[:, :-labels.shape[1]]\n",
    "    shuffled_labels = shuffled.iloc[:, -labels.shape[1]:]\n",
    "    \n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "\n",
    "def preprocessed_data (x_df, y_df) :\n",
    "    #plot_histograms(x_df, title=\"Original Data Distribution\")\n",
    "\n",
    "    x_df, y_df = shuffle_dataset(x_df, y_df)     \n",
    "    x_df = scale_last_columns(x_df)\n",
    "    x_df = remove_low_variance_features_last_columns(x_df)\n",
    "    x_df = apply_pca_last_columns(x_df)\n",
    "\n",
    "\n",
    "    # Final distribution\n",
    "    #plot_histograms(x_df, title=\"Final Data Distribution After Preprocessing\")\n",
    "    \n",
    "    return x_df, y_df\n",
    "\n",
    "# Check for NaN values\n",
    "def has_nan(df):\n",
    "    has_nan = df.isnull().values.any()\n",
    "\n",
    "    if has_nan:\n",
    "        print(\"The DataFrame contains NaN values.\")\n",
    "    else:\n",
    "        print(\"The DataFrame does not contain any NaN values.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre, y_pre = preprocessed_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the DataFrame (original): (792, 347953)\n",
      "Dimensions of the DataFrame (after preprocessing): (792, 334400)\n",
      "\n",
      "Mean of all values (original): 0.04272837026981166\n",
      "Mean of all values (after preprocessing): 0.025537186572903846\n",
      "\n",
      "Max value (original): 188.0 Min value (original): 0.0\n",
      "Max value (after preprocessing): 20.44752993088678 Min value (after preprocessing): -7.194270960737348\n",
      "\n",
      "Standard deviation (original): 0.2734607003296232\n",
      "Standard deviation (after preprocessing): 0.1591887692115655\n",
      "\n",
      "Variance (original): 0.074780754624768\n",
      "Variance (after preprocessing): 0.02534106424309306\n"
     ]
    }
   ],
   "source": [
    "metrics(x, x_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédicteurs prétraités ont été sauvegardés dans le fichier 'X_matrix_preprocessed.csv'.\n",
      "Les cibles prétraitées ont été sauvegardées dans le fichier 'Y_matrix_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder X_pre dans un fichier CSV\n",
    "X_output_file = \"data/X_matrix_preprocessed.csv\"\n",
    "x_pre.to_csv(X_output_file, index=False)\n",
    "print(f\"Les prédicteurs prétraités ont été sauvegardés dans le fichier '{X_output_file}'.\")\n",
    "\n",
    "# Sauvegarder Y_pre dans un fichier CSV\n",
    "Y_output_file = \"data/Y_matrix_preprocessed.csv\"\n",
    "y_pre.to_csv(Y_output_file, index=False)\n",
    "print(f\"Les cibles prétraitées ont été sauvegardées dans le fichier '{Y_output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
