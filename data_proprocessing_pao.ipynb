{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y prends la forme : yeast ID, doubling_time\n",
    "# Charger les datasets\n",
    "x_train = pd.read_csv(\"data/X_matrix.csv\")\n",
    "y_train = pd.read_csv(\"data/Y_matrix.csv\")\n",
    "print(\"csv read complete\")\n",
    "\n",
    "#check if the first columns is the same\n",
    "is_same = x_train.iloc[:, 0].equals(y_train.iloc[:, 0])\n",
    "print(f\"Are the first columns the same? {is_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the DataFrame (original): (792, 348523)\n",
      "Dimensions of the DataFrame (after preprocessing): (792, 334900)\n",
      "\n",
      "Mean of all values (original): 0.03342165073829667\n",
      "Mean of all values (after preprocessing): 0.02587988254763545\n",
      "\n",
      "Max value (original): 1.0 Min value (original): 0.0\n",
      "Max value (after preprocessing): 1.0000000000000002 Min value (after preprocessing): 0.0\n",
      "\n",
      "Standard deviation (original): 0.16990634242416425\n",
      "Standard deviation (after preprocessing): 0.15819315406553178\n",
      "\n",
      "Variance (original): 0.028868165195957352\n",
      "Variance (after preprocessing): 0.025025073993201075\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scale_last_columns(data, num_last_columns=7000):\n",
    "    \"\"\"\n",
    "    Scales the last N columns (assumed to be CNVs) to a range of 0 to 1.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): x_train data\n",
    "        num_last_columns (int): Number of copy number variation columns from the end to scale .\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with the last columns scaled.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    cnv_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Scale these columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_cnv = scaler.fit_transform(cnv_columns)\n",
    "    \n",
    "    # Replace the last N columns with their scaled values\n",
    "    data.iloc[:, -num_last_columns:] = scaled_cnv\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_low_variance_features_last_columns(data, num_last_columns=7000, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Removes features with variance below a specified threshold in the last N columns.\n",
    "    This has been done already for mutations during our extraction of data, so it is only useful to do it for the copy number variation columns.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply variance filtering.\n",
    "        threshold (float): Minimum variance a feature must have to be retained.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with low-variance features removed in the last N columns.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply VarianceThreshold to these columns\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    reduced_data = selector.fit_transform(target_columns)\n",
    "    \n",
    "    # Get the selected column indices\n",
    "    selected_columns = target_columns.columns[selector.get_support()]\n",
    "    \n",
    "    # Replace the last N columns with the reduced set\n",
    "    data = data.drop(columns=target_columns.columns)  # Drop the original last N columns\n",
    "    data = pd.concat([data, pd.DataFrame(reduced_data, columns=selected_columns)], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_pca_last_columns(data, num_last_columns=7000, n_components=0.95, normalize=True):\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce dimensionality of the last N columns of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply PCA.\n",
    "        n_components (float or int): Number of components to keep or the amount of variance to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with PCA applied to the last N columns.\n",
    "    \"\"\"\n",
    "    # Separate the last N columns and the rest of the dataset\n",
    "    other_columns = data.iloc[:, :-num_last_columns]\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply PCA to the last N columns\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(target_columns)\n",
    "\n",
    "    # Normalize the PCA-transformed features if specified\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        reduced_data = scaler.fit_transform(reduced_data)\n",
    "    \n",
    "    # Create a new DataFrame for the reduced PCA data\n",
    "    reduced_columns = [f'PCA_{i+1}' for i in range(reduced_data.shape[1])]\n",
    "    reduced_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=data.index)\n",
    "    \n",
    "    # Concatenate the other columns with the reduced PCA columns\n",
    "    result = pd.concat([other_columns, reduced_df], axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def metrics(x_train, x_train_preprocessed):\n",
    "    \"\"\"\n",
    "    Compute and display various metrics for the original and preprocessed data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (pd.DataFrame): Original data.\n",
    "        x_train_preprocessed (pd.DataFrame): Preprocessed data.\n",
    "    \"\"\"\n",
    "    # Compute and print dimensions\n",
    "    print(\"Dimensions of the DataFrame (original):\", x_train.shape)\n",
    "    print(\"Dimensions of the DataFrame (after preprocessing):\", x_train_preprocessed.shape)\n",
    "\n",
    "    # Exclude the first row and column for calculations\n",
    "    x_cut = x_train.iloc[1:, 1:]\n",
    "    x_cut_p = x_train_preprocessed.iloc[1:, 1:]\n",
    "\n",
    "    # Compute and print mean\n",
    "    mean_value_x_train = x_cut.values.mean()\n",
    "    mean_value_x_train_preprocessed = x_cut_p.values.mean()\n",
    "    print(\"\\nMean of all values (original):\", mean_value_x_train)\n",
    "    print(\"Mean of all values (after preprocessing):\", mean_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print max and min\n",
    "    max_value_x_train = np.max(x_cut.values)\n",
    "    min_value_x_train = np.min(x_cut.values)\n",
    "    max_value_x_train_preprocessed = np.max(x_cut_p.values)\n",
    "    min_value_x_train_preprocessed = np.min(x_cut_p.values)\n",
    "    print(\"\\nMax value (original):\", max_value_x_train, \"Min value (original):\", min_value_x_train)\n",
    "    print(\"Max value (after preprocessing):\", max_value_x_train_preprocessed, \"Min value (after preprocessing):\", min_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print standard deviation\n",
    "    std_x_train = x_cut.values.std()\n",
    "    std_x_train_preprocessed = x_cut_p.values.std()\n",
    "    print(\"\\nStandard deviation (original):\", std_x_train)\n",
    "    print(\"Standard deviation (after preprocessing):\", std_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print variance\n",
    "    var_x_train = x_cut.values.var()\n",
    "    var_x_train_preprocessed = x_cut_p.values.var()\n",
    "    print(\"\\nVariance (original):\", var_x_train)\n",
    "    print(\"Variance (after preprocessing):\", var_x_train_preprocessed)\n",
    "\n",
    "\n",
    "def preprocessed_data (x_df, y_df) :\n",
    "    x_train = x_df\n",
    "    x_train = scale_last_columns(x_train)\n",
    "    x_train = remove_low_variance_features_last_columns(x_train)\n",
    "    x_train = apply_pca_last_columns(x_train)\n",
    "\n",
    "    y_train= y_df\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "x_train_preprocessed, _ = preprocessed_data (x_train, y_train)\n",
    "metrics(x_train, x_train_preprocessed)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
