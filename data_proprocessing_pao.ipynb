{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv read complete\n",
      "Are the first columns the same? True\n"
     ]
    }
   ],
   "source": [
    "# y prends la forme : yeast ID, doubling_time\n",
    "# Charger les datasets\n",
    "x = pd.read_csv(\"data/X_matrix.csv\")\n",
    "y = pd.read_csv(\"data/Y_matrix.csv\")\n",
    "print(\"csv read complete\")\n",
    "\n",
    "#check if the first columns is the same\n",
    "is_same = x.iloc[:, 0].equals(y.iloc[:, 0])\n",
    "print(f\"Are the first columns the same? {is_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(data, title, bins=50, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Plot histograms of all numerical columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    data.hist(bins=bins, figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(data, title, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the correlation matrix.\n",
    "    \"\"\"\n",
    "    corr_matrix = data.corr()\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, annot=False, fmt=\".2f\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_pca_variance(pca, title):\n",
    "    \"\"\"\n",
    "    Plot the explained variance ratio of PCA components.\n",
    "    \"\"\"\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(explained_variance, marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_last_columns(data, num_last_columns=7000):\n",
    "    \"\"\"\n",
    "    Scales the last N columns (assumed to be CNVs) to a range of 0 to 1.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): x_train data\n",
    "        num_last_columns (int): Number of copy number variation columns from the end to scale .\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with the last columns scaled.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    cnv_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Scale these columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_cnv = scaler.fit_transform(cnv_columns)\n",
    "    \n",
    "    # Replace the last N columns with their scaled values\n",
    "    data.iloc[:, -num_last_columns:] = scaled_cnv\n",
    "\n",
    "    # Plot after scaling\n",
    "    #plot_histograms(data.iloc[:, -num_last_columns:], title=\"After Scaling CNVs\")\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_low_variance_features_last_columns(data, num_last_columns=7000, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Removes features with variance below a specified threshold in the last N columns.\n",
    "    This has been done already for mutations during our extraction of data, so it is only useful to do it for the copy number variation columns.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply variance filtering.\n",
    "        threshold (float): Minimum variance a feature must have to be retained.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with low-variance features removed in the last N columns.\n",
    "    \"\"\"\n",
    "    # Select the last N columns\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply VarianceThreshold to these columns\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    reduced_data = selector.fit_transform(target_columns)\n",
    "    \n",
    "    # Get the selected column indices\n",
    "    selected_columns = target_columns.columns[selector.get_support()]\n",
    "    \n",
    "    # Replace the last N columns with the reduced set\n",
    "    data = data.drop(columns=target_columns.columns)  # Drop the original last N columns\n",
    "    data = pd.concat([data, pd.DataFrame(reduced_data, columns=selected_columns)], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Plot variance after filtering\n",
    "    reduced_variances = pd.DataFrame(reduced_data).var(axis=0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(reduced_variances, bins=50, color=\"orange\")\n",
    "    plt.title(\"Variance of Last Columns (After Filtering)\")\n",
    "    plt.xlabel(\"Variance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_pca_last_columns(data, num_last_columns=7000, n_components=0.95, normalize=True):\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce dimensionality of the last N columns of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        num_last_columns (int): Number of columns from the end to apply PCA.\n",
    "        n_components (float or int): Number of components to keep or the amount of variance to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with PCA applied to the last N columns.\n",
    "    \"\"\"\n",
    "    # Separate the last N columns and the rest of the dataset\n",
    "    other_columns = data.iloc[:, :-num_last_columns]\n",
    "    target_columns = data.iloc[:, -num_last_columns:]\n",
    "    \n",
    "    # Apply PCA to the last N columns\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(target_columns)\n",
    "\n",
    "    # Plot explained variance\n",
    "    #visualize_pca_variance(pca, title=\"Explained Variance by PCA Components\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Normalize the PCA-transformed features if specified\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        reduced_data = scaler.fit_transform(reduced_data)\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame for the reduced PCA data\n",
    "    reduced_columns = [f'PCA_{i+1}' for i in range(reduced_data.shape[1])]\n",
    "    reduced_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=data.index)\n",
    "    \n",
    "    # Concatenate the other columns with the reduced PCA columns\n",
    "    result = pd.concat([other_columns, reduced_df], axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def metrics(x_train, x_train_preprocessed):\n",
    "    \"\"\"\n",
    "    Compute and display various metrics for the original and preprocessed data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (pd.DataFrame): Original data.\n",
    "        x_train_preprocessed (pd.DataFrame): Preprocessed data.\n",
    "    \"\"\"\n",
    "    # Compute and print dimensions\n",
    "    print(\"Dimensions of the DataFrame (original):\", x_train.shape)\n",
    "    print(\"Dimensions of the DataFrame (after preprocessing):\", x_train_preprocessed.shape)\n",
    "\n",
    "    # Exclude the first row and column for calculations\n",
    "    x_cut = x_train.iloc[1:, 1:]\n",
    "    x_cut_p = x_train_preprocessed.iloc[1:, 1:]\n",
    "\n",
    "    # Compute and print mean\n",
    "    mean_value_x_train = x_cut.values.mean()\n",
    "    mean_value_x_train_preprocessed = x_cut_p.values.mean()\n",
    "    print(\"\\nMean of all values (original):\", mean_value_x_train)\n",
    "    print(\"Mean of all values (after preprocessing):\", mean_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print max and min\n",
    "    max_value_x_train = np.max(x_cut.values)\n",
    "    min_value_x_train = np.min(x_cut.values)\n",
    "    max_value_x_train_preprocessed = np.max(x_cut_p.values)\n",
    "    min_value_x_train_preprocessed = np.min(x_cut_p.values)\n",
    "    print(\"\\nMax value (original):\", max_value_x_train, \"Min value (original):\", min_value_x_train)\n",
    "    print(\"Max value (after preprocessing):\", max_value_x_train_preprocessed, \"Min value (after preprocessing):\", min_value_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print standard deviation\n",
    "    std_x_train = x_cut.values.std()\n",
    "    std_x_train_preprocessed = x_cut_p.values.std()\n",
    "    print(\"\\nStandard deviation (original):\", std_x_train)\n",
    "    print(\"Standard deviation (after preprocessing):\", std_x_train_preprocessed)\n",
    "\n",
    "    # Compute and print variance\n",
    "    var_x_train = x_cut.values.var()\n",
    "    var_x_train_preprocessed = x_cut_p.values.var()\n",
    "    print(\"\\nVariance (original):\", var_x_train)\n",
    "    print(\"Variance (after preprocessing):\", var_x_train_preprocessed)\n",
    "\n",
    "\n",
    "def shuffle_dataset(data, labels):\n",
    "    \"\"\"\n",
    "    Shuffle the dataset and labels together to randomize the order of examples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Features dataset.\n",
    "        labels (pd.DataFrame): Labels dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Shuffled features and labels.\n",
    "    \"\"\"\n",
    "    combined = pd.concat([data, labels], axis=1)\n",
    "    shuffled = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Separate features and labels again\n",
    "    shuffled_data = shuffled.iloc[:, :-labels.shape[1]]\n",
    "    shuffled_labels = shuffled.iloc[:, -labels.shape[1]:]\n",
    "    \n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "\n",
    "def preprocessed_data (x_df, y_df) :\n",
    "    #plot_histograms(x_df, title=\"Original Data Distribution\")\n",
    "\n",
    "    x_df, y_df = shuffle_dataset(x_df, y_df)     \n",
    "    x_df = scale_last_columns(x_df)\n",
    "    x_df = remove_low_variance_features_last_columns(x_df)\n",
    "    x_df = apply_pca_last_columns(x_df)\n",
    "\n",
    "\n",
    "    # Final distribution\n",
    "    #plot_histograms(x_df, title=\"Final Data Distribution After Preprocessing\")\n",
    "    \n",
    "    return x_df, y_df\n",
    "\n",
    "# Check for NaN values\n",
    "def has_nan(df):\n",
    "    has_nan = df.isnull().values.any()\n",
    "\n",
    "    if has_nan:\n",
    "        print(\"The DataFrame contains NaN values.\")\n",
    "    else:\n",
    "        print(\"The DataFrame does not contain any NaN values.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train_pre, y_train_pre \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mpreprocessed_data\u001b[0;34m(x_df, y_df)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessed_data\u001b[39m (x_df, y_df) :\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m#plot_histograms(x_df, title=\"Original Data Distribution\")\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     x_df, y_df \u001b[38;5;241m=\u001b[39m shuffle_dataset(x_df, y_df)     \n\u001b[0;32m--> 177\u001b[0m     x_df \u001b[38;5;241m=\u001b[39m \u001b[43mscale_last_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     x_df \u001b[38;5;241m=\u001b[39m remove_low_variance_features_last_columns(x_df)\n\u001b[1;32m    179\u001b[0m     x_df \u001b[38;5;241m=\u001b[39m apply_pca_last_columns(x_df)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mscale_last_columns\u001b[0;34m(data, num_last_columns)\u001b[0m\n\u001b[1;32m     17\u001b[0m scaled_cnv \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(cnv_columns)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Replace the last N columns with their scaled values\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39mnum_last_columns:] \u001b[38;5;241m=\u001b[39m scaled_cnv\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Plot after scaling\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#plot_histograms(data.iloc[:, -num_last_columns:], title=\"After Scaling CNVs\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    715\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 716\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1688\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1688\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1727\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_frame_value(indexer, value, name)\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_2d_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ilocs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(pi):\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;66;03m# We are setting multiple rows in a single column.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(ilocs[\u001b[38;5;241m0\u001b[39m], value, pi)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1797\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_2d_value\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value when setting with an ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, loc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ilocs):\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# setting with a list, re-coerces\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1895\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;66;03m# reset the sliced object if unique\u001b[39;00m\n\u001b[0;32m-> 1895\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3814\u001b[0m, in \u001b[0;36mDataFrame._iset_item\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iset_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc: \u001b[38;5;28mint\u001b[39m, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3813\u001b[0m     arraylike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m-> 3814\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marraylike\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   3818\u001b[0m     \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[1;32m   3819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3794\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[1;32m   3791\u001b[0m     \u001b[38;5;28mself\u001b[39m, loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, value, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:1141\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             removed_blknos\u001b[38;5;241m.\u001b[39mappend(blkno_l)\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1141\u001b[0m             \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs[blk\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(blk))\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(removed_blknos):\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m# Remove blocks & update blknos accordingly\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:388\u001b[0m, in \u001b[0;36mBlock.delete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mDelete given loc(-s) from block in-place.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Argument 1 to \"delete\" has incompatible type \"Union[ndarray[Any, Any],\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# ExtensionArray]\"; expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Sequence[_SupportsArray[dtype[Any]]], Sequence[Sequence\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# [_SupportsArray[dtype[Any]]]], Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]], Sequence[Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]]]]\"  [arg-type]\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmgr_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr_locs\u001b[38;5;241m.\u001b[39mdelete(loc)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4555\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4552\u001b[0m         keep[obj,] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   4554\u001b[0m     slobj[axis] \u001b[38;5;241m=\u001b[39m keep\n\u001b[0;32m-> 4555\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   4557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m   4558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(new)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_pre, y_pre = preprocessed_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(x, x_pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
