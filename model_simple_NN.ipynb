{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os.path\n",
    "from preprocessing import *\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import nn, optim\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import EpochScoring\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "np.random.seed(10)\n",
    "\n",
    "X_file = \"data/X_matrix.csv\"\n",
    "Y_file = \"data/Y_matrix.csv\"\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "x_df = pd.read_csv(X_file)\n",
    "y_df = pd.read_csv(Y_file)\n",
    "x_data_f, y_data_f = preprocessed_data(x_df, y_df)\n",
    "\n",
    "#x, y, x_test_try = preprocessed_data(path_train, path_cddd, path_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the data...\n"
     ]
    }
   ],
   "source": [
    "X_file = \"data/X_matrix.csv\"\n",
    "Y_file = \"data/Y_matrix.csv\"\n",
    "\n",
    "print(\"Loading and preprocessing the data...\")\n",
    "\n",
    "# Load the column names (header)\n",
    "column_names = np.genfromtxt(X_file, delimiter=',', max_rows=1, dtype=str)[1:]  # Skip the first column if it's row names\n",
    "\n",
    "# Load the row names (index) from the first column and the data (excluding first column)\n",
    "data = np.loadtxt(X_file, delimiter=',', skiprows=1, usecols=range(1, 348523))\n",
    "row_names = np.loadtxt(X_file, delimiter=',', skiprows=1, usecols=0, dtype=str)\n",
    "\n",
    "# Create the DataFrame\n",
    "x2_df = pd.DataFrame(data, index=row_names, columns=column_names)\n",
    "x2_df = pd.DataFrame(data, columns=column_names)\n",
    "y2_df = pd.read_csv(Y_file)\n",
    "x_data_f, y_data_f = preprocessed_data(x2_df, y2_df)\n",
    "\n",
    "x_data_f = x_data_f.drop(x_data_f.columns[0], axis=1)\n",
    "y_data_f = y_data_f.drop(y_data_f.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: 0.0\n",
      "Maximum value: 1.0000000000000002\n",
      "     YAL001C_1002_P->L  YAL001C_1011_I->V  YAL001C_1022_D->Y  \\\n",
      "AAB                0.0                0.0                0.0   \n",
      "AAC                0.0                0.0                0.0   \n",
      "AAD                0.0                0.0                0.0   \n",
      "AAE                0.0                0.0                0.0   \n",
      "AAG                0.0                0.0                0.0   \n",
      "\n",
      "     YAL001C_1027_V->M  YAL001C_102_K->R  YAL001C_1031_R->L  \\\n",
      "AAB                0.0               0.0                0.0   \n",
      "AAC                0.0               0.0                0.0   \n",
      "AAD                0.0               0.0                0.0   \n",
      "AAE                0.0               0.0                0.0   \n",
      "AAG                0.0               0.0                0.0   \n",
      "\n",
      "     YAL001C_1039_R->K  YAL001C_1047_P->A  YAL001C_1084_G->V  \\\n",
      "AAB                0.0                0.0                0.0   \n",
      "AAC                0.0                0.0                0.0   \n",
      "AAD                0.0                0.0                0.0   \n",
      "AAE                0.0                0.0                0.0   \n",
      "AAG                0.0                0.0                0.0   \n",
      "\n",
      "     YAL001C_1089_N->T  ...    PCA_48    PCA_49    PCA_50    PCA_51    PCA_52  \\\n",
      "AAB                0.0  ...  0.661402  0.539800  0.579331  0.667971  0.486439   \n",
      "AAC                1.0  ...  0.145846  0.675912  0.531005  0.512079  0.315688   \n",
      "AAD                0.0  ...  0.493624  0.451882  0.520064  0.212298  0.281943   \n",
      "AAE                0.0  ...  0.462453  0.619413  0.461029  0.549627  0.711370   \n",
      "AAG                0.0  ...  0.472129  0.448266  0.357119  0.427507  0.397143   \n",
      "\n",
      "       PCA_53    PCA_54    PCA_55    PCA_56    PCA_57  \n",
      "AAB  0.153075  0.652206  0.608388  0.671739  0.084572  \n",
      "AAC  0.024887  0.806674  0.409749  0.415366  0.351482  \n",
      "AAD  0.485520  0.483148  0.455057  0.648086  0.586087  \n",
      "AAE  0.577802  0.486111  0.531214  0.133885  0.337888  \n",
      "AAG  0.307403  0.289848  0.468355  0.585514  0.311213  \n",
      "\n",
      "[5 rows x 341958 columns]\n"
     ]
    }
   ],
   "source": [
    "# Minimum value in the entire DataFrame\n",
    "min_value = x2_df.min().min()\n",
    "\n",
    "# Maximum value in the entire DataFrame\n",
    "max_value = x2_df.max().max()\n",
    "\n",
    "print(f\"Minimum value: {min_value}\")\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "print(x_data_f.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation des données...\n"
     ]
    }
   ],
   "source": [
    "# Préparation des données\n",
    "print(\"Préparation des données...\")\n",
    "\n",
    "# Number of input features\n",
    "n_input_features = x_data_f.shape[1]\n",
    "\n",
    "# Define an enhanced neural network\n",
    "class EnhancedRegressionNet(nn.Module):\n",
    "    def __init__(self, n_input_features, dropout_rate, n_neurons=128):\n",
    "        super(EnhancedRegressionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input_features, n_neurons) #n_input_features\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "# Define scoring callbacks for training and validation loss\n",
    "train_loss = EpochScoring(scoring='neg_mean_squared_error', on_train=True, name='train_loss', lower_is_better=False)\n",
    "valid_loss = EpochScoring(scoring='neg_mean_squared_error', name='valid_loss', lower_is_better=False)\n",
    "\n",
    "\n",
    "#Neural Network Regressor\n",
    "net = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features , #n_input_features\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#parameter grid\n",
    "param_grid = {\n",
    "    'module__dropout_rate': [0.01, 0.011, 0.012],\n",
    "    'lr': [0.00013, 0.00015, 0.00017],\n",
    "    'max_epochs': [10, 100, 1000],\n",
    "    'optimizer': [optim.Adam],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GridSearchCV \n",
    "grid_search = GridSearchCV(net, param_grid=param_grid, cv=KFold(n_splits=6), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_data_f.values.astype(np.float32), y_data_f.values.astype(np.float32))\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Training of the model\n",
    "best_net = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features,\n",
    "    module__n_neurons=128,\n",
    "    module__dropout_rate=best_params['module__dropout_rate'],\n",
    "    criterion=nn.MSELoss,\n",
    "    max_epochs=best_params['max_epochs'],\n",
    "    optimizer=best_params['optimizer'],\n",
    "    lr=best_params['lr'],\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5), train_loss, valid_loss],\n",
    "    verbose=1\n",
    ")\n",
    "best_net.fit(x_data_f.values.astype(np.float32), y_data_f.values.astype(np.float32))\n",
    "\n",
    "Y_pred = best_net.predict(x_data_f.values.astype(np.float32))\n",
    "\n",
    "id_array = np.arange(1, len(Y_pred)+1)\n",
    "final_df = pd.DataFrame({\n",
    "    'ID': id_array,\n",
    "    'division_rate': Y_pred.flatten()\n",
    "})\n",
    "\n",
    "#pour que ça run tout en même temps\n",
    "mse = mean_squared_error(y_data_f, Y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "r2 = r2_score(y_data_f, Y_pred)\n",
    "print(f'R2 score: {r2}')\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "final_csv = final_df.to_csv(\"Data\\\\results_nn3.csv\", index=False)\n",
    "\n",
    "# Extract training and validation loss for a plot\n",
    "train_losses = best_net.history[:, 'train_loss']\n",
    "valid_losses = best_net.history[:, 'valid_loss']\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Negative Mean Squared Error')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"Data\\\\NNplot_nn3.png\")\n",
    "\n",
    "'''\n",
    "In a neural network, you don't directly get \"feature importances\" like in tree-based models (e.g., Random Forest or XGBoost). \n",
    "However, you can estimate feature importance by analyzing how sensitive the model's predictions are to changes in each feature. \n",
    "This method is often referred to as \"permutation importance\" or \"feature sensitivity analysis.\"\n",
    "\n",
    "Here's a Python script to compute and visualize the top 10 most important features based on permutation importance:\n",
    "'''\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "results = permutation_importance(\n",
    "    best_net,  # Trained model\n",
    "    x_data_f.values.astype(np.float32),  # Input data\n",
    "    y_data_f.values.astype(np.float32),  # Target values\n",
    "    scoring=\"neg_mean_squared_error\",  # Scoring metric\n",
    "    n_repeats=10,  # Number of permutations\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": x_data_f.columns,\n",
    "    \"Importance\": results.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Select the top 10 features\n",
    "top_10_features = feature_importances.head(10)\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_features[\"Feature\"], top_10_features[\"Importance\"], align=\"center\")\n",
    "plt.gca().invert_yaxis()  # Highest importance on top\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Data\\\\Feature_Importance_Plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mr2\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[0;32m      3\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_data_f1, Y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_data_f1, Y_pred)\n",
    "r2 = r2_score(y_data_f1, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In a neural network, you don't directly get \"feature importances\" like in tree-based models (e.g., Random Forest or XGBoost). \n",
    "However, you can estimate feature importance by analyzing how sensitive the model's predictions are to changes in each feature. \n",
    "This method is often referred to as \"permutation importance\" or \"feature sensitivity analysis.\"\n",
    "\n",
    "Here's a Python script to compute and visualize the top 10 most important features based on permutation importance:\n",
    "'''\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "results = permutation_importance(\n",
    "    best_net,  # Trained model\n",
    "    x_data_f1.values.astype(np.float32),  # Input data\n",
    "    y_data_f1.values.astype(np.float32),  # Target values\n",
    "    scoring=\"neg_mean_squared_error\",  # Scoring metric\n",
    "    n_repeats=10,  # Number of permutations\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": x_data_f1.columns,\n",
    "    \"Importance\": results.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Select the top 10 features\n",
    "top_10_features = feature_importances.head(10)\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_features[\"Feature\"], top_10_features[\"Importance\"], align=\"center\")\n",
    "plt.gca().invert_yaxis()  # Highest importance on top\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Data\\\\Feature_Importance_Plot.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
