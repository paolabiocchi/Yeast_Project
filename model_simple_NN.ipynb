{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paola\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os.path\n",
    "from preprocessing import *\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import nn, optim\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import EpochScoring\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "X_file = \"data/X_matrix.csv\"\n",
    "Y_file = \"data/Y_matrix.csv\"\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "x_df = pd.read_csv(X_file)\n",
    "y_df = pd.read_csv(Y_file)\n",
    "x_data_f, y_data_f = preprocessed_data(x_df, y_df)\n",
    "\n",
    "#x, y, x_test_try = preprocessed_data(path_train, path_cddd, path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation des données...\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0245\u001b[0m        \u001b[32m0.1256\u001b[0m  2.5141\n",
      "      2        \u001b[36m0.1261\u001b[0m        \u001b[32m0.1200\u001b[0m  2.1881\n",
      "      3        \u001b[36m0.0751\u001b[0m        0.1262  2.2571\n",
      "      4        \u001b[36m0.0528\u001b[0m        0.1264  2.2749\n",
      "      5        \u001b[36m0.0441\u001b[0m        \u001b[32m0.1158\u001b[0m  2.2405\n",
      "      6        0.0531        0.1207  2.2026\n",
      "      7        \u001b[36m0.0432\u001b[0m        \u001b[32m0.1147\u001b[0m  2.2502\n",
      "      8        \u001b[36m0.0368\u001b[0m        0.1224  2.3011\n",
      "      9        0.0410        0.1149  2.1305\n",
      "     10        \u001b[36m0.0359\u001b[0m        0.1173  2.1480\n",
      "     11        \u001b[36m0.0291\u001b[0m        0.1279  2.1705\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Best Parameters: {'lr': 0.0004, 'max_epochs': 70, 'module__dropout_rate': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'module_dropout_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Training of the model\u001b[39;00m\n\u001b[0;32m     64\u001b[0m best_net \u001b[38;5;241m=\u001b[39m NeuralNetRegressor(\n\u001b[0;32m     65\u001b[0m     module\u001b[38;5;241m=\u001b[39mEnhancedRegressionNet,\n\u001b[0;32m     66\u001b[0m     module__n_input_features\u001b[38;5;241m=\u001b[39mn_input_features,\n\u001b[0;32m     67\u001b[0m     module__n_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m---> 68\u001b[0m     module_dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodule_dropout_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     69\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss,\n\u001b[0;32m     70\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     71\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     72\u001b[0m     lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     73\u001b[0m     iterator_train__shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m), train_loss, valid_loss],\n\u001b[0;32m     75\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m best_net\u001b[38;5;241m.\u001b[39mfit(x_data_f1\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), y_data_f1\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     79\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m best_net\u001b[38;5;241m.\u001b[39mpredict(x_data_f1\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'module_dropout_rate'"
     ]
    }
   ],
   "source": [
    "# Préparation des données\n",
    "print(\"Préparation des données...\")\n",
    "x_data_f1 = x_data_f.drop(columns=[\"Yeast_ID\"]).fillna(0)  # Remplacer les valeurs manquantes par 0 dans X\n",
    "y_data_f1 = y_data_f[\"YPD_doublingtime\"].fillna(y_data_f[\"YPD_doublingtime\"].mean())  # Remplacer les valeurs manquantes par la moyenne dans Y\n",
    "\n",
    "# Number of input features\n",
    "n_input_features = x_data_f1.shape[1]\n",
    "\n",
    "# Define an enhanced neural network\n",
    "class EnhancedRegressionNet(nn.Module):\n",
    "    def __init__(self, n_input_features, dropout_rate, n_neurons=128):\n",
    "        super(EnhancedRegressionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input_features, n_neurons) #n_input_features\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "# Define scoring callbacks for training and validation loss\n",
    "train_loss = EpochScoring(scoring='neg_mean_squared_error', on_train=True, name='train_loss', lower_is_better=False)\n",
    "valid_loss = EpochScoring(scoring='neg_mean_squared_error', name='valid_loss', lower_is_better=False)\n",
    "\n",
    "\n",
    "#Neural Network Regressor\n",
    "net = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features , #n_input_features\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#parameter grid\n",
    "param_grid = {\n",
    "    'module_dropout_rate': [0.008, 0.009, 0.010],\n",
    "    'lr': [0.0004, 0.0005, 0.0006],\n",
    "    'max_epochs': [65, 70, 75],\n",
    "    'optimizer': [optim.Adam],\n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV \n",
    "grid_search = GridSearchCV(net, param_grid=param_grid, cv=KFold(n_splits=6), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_data_f1.values.astype(np.float32), y_data_f1.values.astype(np.float32))\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Training of the model\n",
    "best_net = NeuralNetRegressor(\n",
    "    module=EnhancedRegressionNet,\n",
    "    module__n_input_features=n_input_features,\n",
    "    module__n_neurons=128,\n",
    "    module_dropout_rate=best_params['module_dropout_rate'],\n",
    "    criterion=nn.MSELoss,\n",
    "    max_epochs=best_params['max_epochs'],\n",
    "    optimizer=best_params['optimizer'],\n",
    "    lr=best_params['lr'],\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5), train_loss, valid_loss],\n",
    "    verbose=1\n",
    ")\n",
    "best_net.fit(x_data_f1.values.astype(np.float32), y_data_f1.values.astype(np.float32))\n",
    "\n",
    "Y_pred = best_net.predict(x_data_f1.values.astype(np.float32))\n",
    "\n",
    "# Evaluate the model for the train data\n",
    "mse = mean_squared_error(y_data_f1, Y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "id_array = np.arange(1, len(Y_pred)+1)\n",
    "final_df = pd.DataFrame({\n",
    "    'ID': id_array,\n",
    "    'division_rate': Y_pred.flatten()\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "final_csv = final_df.to_csv(\"Data\\\\results_nn3.csv\", index=False)\n",
    "\n",
    "\n",
    "# Extract training and validation loss for a plot\n",
    "train_losses = best_net.history[:, 'train_loss']\n",
    "valid_losses = best_net.history[:, 'valid_loss']\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Negative Mean Squared Error')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"Data\\\\NNplot_nn3.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
